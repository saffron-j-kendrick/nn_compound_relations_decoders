{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import textdistance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "from itertools import permutations, product, combinations\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import ttest_ind, ttest_rel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Volumes/My Passport/NOUN-NOUN-COMPOUNDS-V1/data/noun_noun_compounds/nn_compounds_with_continuation_words1.xlsx\")\n",
    "df = df[~df[\"AND_sentence\"].isnull()]\n",
    "corr_metric = \"kendalltau\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = dict(zip([\"bert-base-uncased\", \"roberta-base\", \"openai-community/gpt2\", \"microsoft/phi-1\", \"meta-llama/Llama-3.2-1B\", \"bert-base-japanese\", \"distilroberta\", \"xlm-mlm-xnli15-1024\", \"xlnet-base-cased\", \"openai-community/openai-gpt\"], range(0,10)))\n",
    "\n",
    "\n",
    "model_name_map = {\"bert-base-uncased\" : \"BERT\", \"roberta-base\" : \"RoBERTa\", \"openai-community/gpt2\" : \"GPT2\", \"microsoft/phi-1\" : \"Phi1\", \"meta-llama/Llama-3.2-1B\" : \"LLaMA3\", \"bert-base-japanese\" : \"BERT-Japanese\", \"distilroberta\" : \"DistilRoBERTa\", \"xlm-mlm-xnli15-1024\" : \"XLM\", \"xlnet-base-cased\" : \"XLNet\", \"openai-community/openai-gpt\" : \"GPT1\"}\n",
    "\n",
    "def sort_df_by_model_order(df, keep_order_col = True, update_names = True):\n",
    "    df[\"model_order\"] = [order_dict[x.model] for x in df.iloc]\n",
    "    extra_columns_to_sort = [\"representation\", \"Word representations processed . . .\"]\n",
    "    extra_columns_to_sort = [x for x in extra_columns_to_sort if x in df.columns]\n",
    "    sort_cols = [\"model_order\"] + extra_columns_to_sort\n",
    "    df = df.sort_values(sort_cols)\n",
    "\n",
    "    if not keep_order_col:\n",
    "        del df[\"model_order\"]\n",
    "    \n",
    "    if update_names:\n",
    "        df[\"model_name\"] = [model_name_map[x.model] for x in df.iloc]\n",
    "        if \"representation\" in df.columns:\n",
    "            df[\"representation_name\"] = [model_name_map[x.representation] if x.representation in model_name_map else x.representation for x in df.iloc]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_sentence(word, original_sent, head_word=True):\n",
    "    if head_word and 'They are' in original_sent:\n",
    "        return 'They are ' + word\n",
    "    if 'It is a' not in original_sent and 'They are' not in original_sent:\n",
    "        return 'It is ' + word\n",
    "    if word[0] in ['a', 'e', 'u', 'i', 'o']:\n",
    "        return 'It is an ' + word\n",
    "    else:\n",
    "        return 'It is a ' + word\n",
    "\n",
    "\n",
    "full_sents = df['AND_sentence'][:300].tolist()\n",
    "\n",
    "words_per_sent = [x.split(' ') for x in full_sents]\n",
    "\n",
    "raw_mod_word_sents = ['{}\\t{}'.format(words[-3], get_processed_sentence(words[-3], full, False)) for words, full in zip(words_per_sent, full_sents)]\n",
    "raw_head_word_sents = ['{}\\t{}'.format(words[-2], get_processed_sentence(words[-2], full, True)) for words, full in zip(words_per_sent, full_sents)]\n",
    "raw_and_word_sents = ['{}\\t{}'.format(words[-1], get_processed_sentence(words[-1], full, True)) for words, full in zip(words_per_sent, full_sents)]\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in list(zip(raw_mod_word_sents, raw_head_word_sents, raw_and_word_sents)) for item in sublist]\n",
    "\n",
    "file = open('/Volumes/My Passport/NOUN-NOUN-COMPOUNDS-V1/data/noun_noun_compounds/composition/probe_words_and_sentences_and_300_raw.txt', 'w')\n",
    "for i, sent in enumerate(flat_list):\n",
    "    file.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def any_sentences_are_different(sentences):\n",
    "    results = [[sent_a == sent_b for sent_b in sentences] for sent_a in sentences]\n",
    "    return any([not all(x) for x in results])\n",
    "\n",
    "any_sentences_are_different(['This is a test!', 'This is a test!', 'This is a test!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/saffronkendrick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(df['NN sentence'][:300].tolist() + df['Gloss sentence'].tolist())\n",
    "\n",
    "df['compound'] = ['{} {}'.format(x['mod'], x['head']) for x in df.iloc]\n",
    "compounds = np.array(df['compound'][:300].tolist() + df['compound'].tolist())\n",
    "\n",
    "process_sent = lambda x: [y for y in nltk.word_tokenize(x.strip().lower()) if y.isalpha()]\n",
    "words_per_sents = [process_sent(x) for x in sentences]\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "word_dict = {'gestates': \"gestate\"}\n",
    "look_up = lambda word: word_dict[word] if word in word_dict else lemmatiser.lemmatize(word)\n",
    "get_vector = lambda word: fasttext[word] if word in fasttext else fasttext[look_up(word)] \n",
    "\n",
    "def get_average_vector(words):\n",
    "    return np.vstack([gest_vector(x) for x in words]).mean(axis=0)\n",
    "\n",
    "paraphrase_ind_tuples = [[i, i+300, i+600] for i in range(300)]\n",
    "paraphrase_inds = [item for sublist in paraphrase_ind_tuples for item in sublist]\n",
    "\n",
    "ordered_sentences = sentences[paraphrase_inds]\n",
    "\n",
    "# # Keep True because this file exists\n",
    "# load = True\n",
    "\n",
    "# if not load:\n",
    "#     fasttext = gensim.models.KeyedVectors.load_word2vec_format('D:/NOUN-NOUN-COMPOUNDS-V1/data/wiki.en.vec', limit=500000)\n",
    "#     mean_fasttext_reps_per_sent = np.vstack([get_average_vector(x) for x in words_per_sent])\n",
    "#     ordered_fasttext_reps = mean_fasttext_reps_per_sent[paraphrase_inds]\n",
    "#     np.save('/Volumes/My Passport/NOUN-NOUN-COMPOUNDS-V1/data/ordered_fasttext_reps_and.npy', ordered_fasttext_reps)\n",
    "# else:\n",
    "#     ordered_fasttext_reps = np.load('/Volumes/My Passport/NOUN-NOUN-COMPOUNDS-V1/data/ordered_fasttext_reps_and.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_per_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
