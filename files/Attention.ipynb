{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fcc5ebd4545b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/black_box_nlp_semantic_similarity/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dependency}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/black_box_nlp_semantic_similarity/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0m__mkl_version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{MajorVersion}.{UpdateVersion}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import textdistance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "import rsa, data_utils, model_utils, representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_utils.get_noun_noun_compound_sentences(data_loc='../data')\n",
    "mod_head_words_per_sentence = data_utils.get_noun_noun_mod_head_words_per_sentence(data_loc='../data')\n",
    "corrected_form_compounds_per_sentence = data_utils.load_corrected_form_compounds_per_sentence(data_loc='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase_ind_tuples = [[i, i+300, i+600] for i in range(300)]\n",
    "# paraphrase_inds = [item for sublist in paraphrase_ind_tuples for item in sublist]\n",
    "\n",
    "# ordered_sentences = sentences[paraphrase_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_utils.dev_model_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "model_input_id_dict = {}\n",
    "model_tokens_dict = {}\n",
    "model_head_noun_locs_per_sent = {}\n",
    "model_mod_word_locs_per_sent = {}\n",
    "\n",
    "for model_name in tqdm.tqdm(models):\n",
    "# for model_name in [model_name]:\n",
    "    if load:\n",
    "        pass\n",
    "    else:\n",
    "        model, tokeniser = model_utils.load_model(model_name)\n",
    "\n",
    "        unpack_dict = lambda x: (x['input_ids'], x['attention_mask'])\n",
    "        input_ids, attention_mask = unpack_dict(tokeniser.batch_encode_plus(sentences, max_length=512, return_tensors='pt', pad_to_max_length=True))\n",
    "        \n",
    "        pad_token_mask = lambda x: np.array(x.cpu() == tokeniser.pad_token_id)\n",
    "        get_tokens_to_keep = lambda x: np.argwhere((pad_token_mask(x) == False)).reshape(-1)\n",
    "        decode_tokens = lambda x: [tokeniser.decode(token).replace(' ', '') for token in x[get_tokens_to_keep(x)].tolist()]\n",
    "        \n",
    "        input_ids_to_keep = [x[get_tokens_to_keep(x)] for x in input_ids]\n",
    "        model_input_id_dict[model_name] = input_ids_to_keep\n",
    "        \n",
    "        model_tokens_dict[model_name] = [decode_tokens(x) for x in input_ids]\n",
    "        \n",
    "        compounds_per_sample = np.array(corrected_form_compounds_per_sentence)\n",
    "        if 'xlm' in model_name:\n",
    "            head_noun_input_ids_per_sent_raw = np.array(tokeniser.batch_encode_plus(compounds_per_sample[:, 1])['input_ids'])\n",
    "            mod_word_input_ids_per_sent_raw = np.array(tokeniser.batch_encode_plus(compounds_per_sample[:, 0])['input_ids'])\n",
    "            \n",
    "            head_noun_input_ids_per_sent = [np.array(x[1:-1]).reshape(-1, 1) for x in head_noun_input_ids_per_sent_raw]\n",
    "            mod_word_input_ids_per_sent = [np.array(x[1:-1]).reshape(-1, 1) for x in mod_word_input_ids_per_sent_raw]\n",
    "        else:\n",
    "            head_noun_input_ids_per_sent_raw = np.array(tokeniser.batch_encode_plus([' ' + x for x in compounds_per_sample[:, 1]])['input_ids'])\n",
    "            mod_word_input_ids_per_sent_raw = np.array(tokeniser.batch_encode_plus([' ' + x for x in compounds_per_sample[:, 0]])['input_ids'])\n",
    "            \n",
    "            # Remove special tokens\n",
    "            non_special_token_mask = lambda x: np.array(tokeniser.get_special_tokens_mask(x, already_has_special_tokens=True)) == 0\n",
    "            get_tokens_to_keep = lambda x: np.argwhere(non_special_token_mask(x)).reshape(-1)\n",
    "            head_noun_input_ids_per_sent = [np.array(x)[get_tokens_to_keep(x)] for x in head_noun_input_ids_per_sent_raw]\n",
    "            mod_word_input_ids_per_sent = [np.array(x)[get_tokens_to_keep(x)] for x in mod_word_input_ids_per_sent_raw]\n",
    "\n",
    "        head_noun_locs_per_sent = [representations.search_sequence_numpy(input_ids_to_keep[i].cpu().numpy().reshape(-1), x.reshape(-1)) for i, x in enumerate(head_noun_input_ids_per_sent)]\n",
    "        mod_word_locs_per_sent = [representations.search_sequence_numpy(input_ids_to_keep[i].cpu().numpy().reshape(-1), x.reshape(-1)) for i, x in enumerate(mod_word_input_ids_per_sent)]\n",
    "        \n",
    "        model_head_noun_locs_per_sent[model_name] = np.array(head_noun_locs_per_sent)\n",
    "        model_mod_word_locs_per_sent[model_name] = np.array(mod_word_locs_per_sent)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x) for i, x in enumerate(sentences) if 'allerg' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_per_sample(model, layer):\n",
    "    file_name = \"../data/representations/nn_compounds_attention/{}_layer_{}_noun_noun_compounds_attention.npy\".format(model, layer)\n",
    "\n",
    "    flat_attention_per_sample = [x[np.argwhere(x != -1)] for x in np.load(file_name)]\n",
    "    attention_per_sample = [x.reshape((int(np.sqrt(x.size)), int(np.sqrt(x.size)))) for x in flat_attention_per_sample]\n",
    "    \n",
    "    return attention_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, labels, ax=None, title=''):\n",
    "    g = sns.heatmap(attention, cmap=\"viridis\", ax=ax)\n",
    "    g.set_yticklabels(labels, rotation=0)\n",
    "    g.set_xticklabels(labels, rotation=45)\n",
    "    if ax != None:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_over_layers(model_name, sample_i, layers=[1, 4, 8, 12]):\n",
    "    fig, axs = plt.subplots(ncols=len(layers), figsize=(6*len(layers), 6))\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        if 'distil' in model_name and layer > 6:\n",
    "            continue\n",
    "        plt.cla()\n",
    "        attention_per_sample = get_attention_per_sample(model_name, layer)\n",
    "        plot_attention(attention_per_sample[sample_i], model_tokens_dict[model_name][sample_i], ax=axs[i], title='model={}, layer={}'.format(model_name, layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_i = 165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('roberta-base', sample_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('bert-base-uncased', sample_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlnet-base-cased', sample_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlm-mlm-xnli15-1024', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlm-mlm-xnli15-1024', sample_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Attention Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer_attention_dict = {}\n",
    "\n",
    "for model_name in model_utils.dev_model_configs.keys():\n",
    "    \n",
    "    model_layer_attention_dict[model_name] = {}\n",
    "    \n",
    "    for layer_i in range(1, 13):\n",
    "        if layer_i > 6 and 'distil' in model_name:\n",
    "            continue\n",
    "        attention_per_sample = get_attention_per_sample(model_name, layer_i)\n",
    "        model_layer_attention_dict[model_name][layer_i] = attention_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "input_ids = model_input_id_dict[model_name]\n",
    "tokens = model_tokens_dict[model_name]\n",
    "head_noun_locs_per_sent = model_head_noun_locs_per_sent[model_name]\n",
    "mod_word_locs_per_sent = model_mod_word_locs_per_sent[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[0]\n",
    "sentence_tokens = tokens[0]\n",
    "sentence_input_ids = input_ids[0]\n",
    "sentence_head_noun_locs_per_sent = head_noun_locs_per_sent[0]\n",
    "sentence_mod_word_locs_per_sent = mod_word_locs_per_sent[0]\n",
    "sentence_attention = model_layer_attention_dict[model_name][1][0]\n",
    "\n",
    "print(sentence)\n",
    "print(sentence_tokens)\n",
    "print(sentence_input_ids)\n",
    "print(sentence_head_noun_locs_per_sent)\n",
    "print(sentence_mod_word_locs_per_sent)\n",
    "\n",
    "plot_attention(sentence_attention, sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating how we can select within the attention matrix using masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_attention = sentence_attention[sentence_head_noun_locs_per_sent]\n",
    "head_head_attention = head_attention[:, sentence_head_noun_locs_per_sent]\n",
    "\n",
    "head_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "head_mask[sentence_head_noun_locs_per_sent] = True\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(10, 10))\n",
    "axs[0].imshow(head_mask)\n",
    "axs[1].imshow(head_mask & head_mask.T)\n",
    "axs[2].imshow(head_mask & ~head_mask.T)\n",
    "axs[0].set_title('head_mask')\n",
    "axs[1].set_title('head_mask & head_mask.T')\n",
    "axs[2].set_title('head_mask & ~head_mask.T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "head_mask[sentence_head_noun_locs_per_sent] = True\n",
    "\n",
    "mod_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "mod_mask[sentence_mod_word_locs_per_sent] = True\n",
    "\n",
    "compound_mask = mod_mask | head_mask\n",
    "plt.imshow(compound_mask & compound_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_mask[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_proportion_in_masks(sentence_attention, mask_a, mask_b):\n",
    "    # e.g mask_a = head_mask, mask_b = mod_mask will select proportion of head_attention within modifier words\n",
    "    # Spans are number of tokens the mask spans\n",
    "    mask_a_span = mask_a[:, 0].sum()\n",
    "    mask_b_span = mask_b[:, 0].sum()\n",
    "    return np.mean(sentence_attention[mask_a & mask_b.T].reshape(mask_a_span, mask_b_span).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_attention[head_mask & head_mask.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_proportion_in_masks(sentence_attention, head_mask, head_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working out compound/multi-token attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the modifier word is at index 3 and the head noun is at index 4\n",
    "sentence_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention for compound tokens\n",
    "sentence_attention[compound_mask].reshape(-1, sentence_attention.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound-compound attention, same as sentence_attention[compound_mask & compound_mask.T].reshape(2, 2) below\n",
    "sentence_attention[compound_mask].reshape(-1, sentence_attention.shape[0])[:, 3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_mask & compound_mask.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_attention[compound_mask & compound_mask.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_attention[compound_mask & compound_mask.T].reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "sentence_attention[compound_mask].reshape(-1, sentence_attention.shape[0]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of attention in each compound token that is within the whole compound\n",
    "sentence_attention[compound_mask & compound_mask.T].reshape(2, 2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average compound-compound attention\n",
    "np.mean(sentence_attention[compound_mask & compound_mask.T].reshape(2, 2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_proportion_in_masks(sentence_attention, compound_mask, compound_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_dicts(model_name):\n",
    "    mask_dicts = []\n",
    "    \n",
    "    for sample_i in range(len(sentences)):\n",
    "        sentence_attention = model_layer_attention_dict[model_name][1][sample_i]\n",
    "        \n",
    "        head_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "        head_mask[model_head_noun_locs_per_sent[model_name][sample_i]] = True\n",
    "\n",
    "        mod_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "        mod_mask[model_mod_word_locs_per_sent[model_name][sample_i]] = True\n",
    "\n",
    "        compound_mask = mod_mask | head_mask\n",
    "        \n",
    "        mask_dicts.append({'head': head_mask, 'modifier': mod_mask, 'compound': compound_mask})\n",
    "    \n",
    "    return mask_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sentence_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dicts = []\n",
    "\n",
    "for sample_i in range(len(sentences)):\n",
    "    sentence_attention = model_layer_attention_dict[model_name][1][sample_i]\n",
    "\n",
    "    head_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "    head_mask[model_head_noun_locs_per_sent[model_name][sample_i]] = True\n",
    "\n",
    "    mod_mask = np.zeros(sentence_attention.shape,dtype=bool)\n",
    "    mod_mask[model_mod_word_locs_per_sent[model_name][sample_i]] = True\n",
    "\n",
    "    compound_mask = mod_mask | head_mask\n",
    "\n",
    "    mask_dicts.append({'head': head_mask, 'modifier': mod_mask, 'compound': compound_mask})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating attention proportion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_feature(name, attention, mask_dict):\n",
    "    attention_target, attention_source = name.split('_')\n",
    "    return attention_proportion_in_masks(attention, mask_dict[attention_target], mask_dict[attention_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_name_dict = ['compound', \"head\", \"modifier\"]\n",
    "mask_name_dict = [\"head\", \"modifier\"]\n",
    "\n",
    "# Each function will take an attention matrix and a dict of masks for that sample, and select and apply the appropriate masks to calculate the appropriate proportion\n",
    "feature_names = []\n",
    "\n",
    "for mask_a_name in mask_name_dict:\n",
    "    for mask_b_name in mask_name_dict:\n",
    "        feature_names.append('{}_{}'.format(mask_a_name, mask_b_name))\n",
    "        \n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer_feature_dict = {}\n",
    "\n",
    "for model_name in model_layer_attention_dict.keys():\n",
    "# for model_name in ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']:\n",
    "    print(model_name)\n",
    "    model_layer_feature_dict[model_name] = {}\n",
    "    \n",
    "    mask_dicts = get_mask_dicts(model_name)\n",
    "    \n",
    "    for layer in tqdm.tqdm(model_layer_attention_dict[model_name].keys()):\n",
    "        model_layer_feature_dict[model_name][layer] = {}\n",
    "        \n",
    "        for feature in feature_names:\n",
    "            model_layer_feature_dict[model_name][layer][feature] = np.mean([calculate_attention_feature(feature, model_layer_attention_dict[model_name][layer][sample_i], mask_dicts[sample_i]) for sample_i in range(len(sentences))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sentence_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(model_layer_feature_dict['bert-base-uncased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for model_name in model_layer_attention_dict.keys():\n",
    "# for model_name in ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']:\n",
    "    print(model_name)\n",
    "    \n",
    "    mask_dicts = get_mask_dicts(model_name)\n",
    "    \n",
    "    for layer in tqdm.tqdm(model_layer_attention_dict[model_name].keys()):\n",
    "        \n",
    "        for feature in feature_names:\n",
    "            feature_mean_val = np.mean([calculate_attention_feature(feature, model_layer_attention_dict[model_name][layer][sample_i], mask_dicts[sample_i]) for sample_i in range(len(sentences))])\n",
    "            rows.append({'model': model_name, 'layer': layer, 'feature': feature, 'mean_attention_proportion': feature_mean_val, 'representation_target': feature.split('_')[0], 'representation_source': feature.split('_')[1]})\n",
    "\n",
    "feature_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'bert-base-uncased'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'roberta-base'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'xlnet-base-cased'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'xlm-mlm-xnli15-1024'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'distilroberta-base'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.lineplot(x='layer', y='mean_attention_proportion', hue='representation_target', style='representation_source', markers=True, data=feature_df[feature_df.model == 'xlm-mlm-xnli15-1024'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/results/nn_compound_transformer_relations_per_word.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "sns.lineplot(x='layer',y='mean_attention_proportion', style='feature', hue='model', data=feature_df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_selection = 'start' # ['all', 'middle', 'not_middle', 'start', 'end']\n",
    "start_layer_boundary = 2\n",
    "end_layer_boundary = 10\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name in list(model_utils.dev_model_configs.keys()):\n",
    "    for layer_selection in ['all', 'middle', 'not_middle', 'start', 'end']:\n",
    "        for experimental_condition in [x for x in df.columns if 'corr' in x]:\n",
    "            for attention_feature in feature_df.feature.unique():\n",
    "                for representation in ['compound_mean', 'head_noun_mean', 'mod_word_mean']:\n",
    "                    if layer_selection == 'all':\n",
    "                        attention_feature_values = feature_df.sort_values(['model', 'layer'])[(feature_df.model == model_name) & (feature_df.feature==attention_feature)].mean_attention_proportion.tolist()\n",
    "                        transformer_same_relation_corr_str_for_rep = df.sort_values(['model', 'layer'])[(df.model == model_name) & (df.representation==representation)][experimental_condition].tolist()\n",
    "                    if layer_selection == 'middle':\n",
    "                        attention_feature_values = feature_df.sort_values(['model', 'layer'])[(feature_df.model == model_name) & (feature_df.feature==attention_feature) & ((feature_df.layer > start_layer_boundary) & (feature_df.layer <= end_layer_boundary))].mean_attention_proportion.tolist()\n",
    "                        transformer_same_relation_corr_str_for_rep = df.sort_values(['model', 'layer'])[((df.model == model_name) & (df.representation==representation) & ((df.layer > start_layer_boundary) & (df.layer <= end_layer_boundary)))][experimental_condition].tolist()\n",
    "                    if layer_selection == 'not_middle':\n",
    "                        attention_feature_values = feature_df.sort_values(['model', 'layer'])[(feature_df.model == model_name) & (feature_df.feature==attention_feature) & ((feature_df.layer <= start_layer_boundary) | (feature_df.layer > end_layer_boundary))].mean_attention_proportion.tolist()\n",
    "                        transformer_same_relation_corr_str_for_rep = df.sort_values(['model', 'layer'])[((df.model == model_name) & (df.representation==representation) & ((df.layer <= start_layer_boundary) | (df.layer > end_layer_boundary)))][experimental_condition].tolist()\n",
    "                    if layer_selection == 'start':\n",
    "                        attention_feature_values = feature_df.sort_values(['model', 'layer'])[(feature_df.model == model_name) & (feature_df.feature==attention_feature) & (feature_df.layer <= start_layer_boundary)].mean_attention_proportion.tolist()\n",
    "                        transformer_same_relation_corr_str_for_rep = df.sort_values(['model', 'layer'])[(df.model == model_name) & (df.representation==representation) & (df.layer <= start_layer_boundary)][experimental_condition].tolist()\n",
    "                    if layer_selection == 'end':\n",
    "                        attention_feature_values = feature_df.sort_values(['model', 'layer'])[(feature_df.model == model_name) & (feature_df.feature==attention_feature) & (feature_df.layer > end_layer_boundary)].mean_attention_proportion.tolist()\n",
    "                        transformer_same_relation_corr_str_for_rep = df.sort_values(['model', 'layer'])[(df.model == model_name) & (df.representation==representation) & (df.layer > end_layer_boundary)][experimental_condition].tolist()\n",
    "\n",
    "                    corr, p_val = spearmanr(attention_feature_values, transformer_same_relation_corr_str_for_rep)\n",
    "\n",
    "                    rows.append({'attention_feature': attention_feature, 'model': model_name, 'representation': representation, 'experimental_condition': experimental_condition, 'corr': corr, 'p_val': p_val, 'layer_selection': layer_selection})\n",
    "\n",
    "results_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see positive correlations between compound attention and the representation of thematic relation within compound groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_self_attention_feature = lambda row: row.attention_feature.split('_')[0] == row.attention_feature.split('_')[1]\n",
    "results_df['self_attention_feature'] = [is_self_attention_feature(x) for x in results_df.iloc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr') & (results_df.layer_selection=='all')]\n",
    "del new_df['experimental_condition']\n",
    "del new_df['layer_selection']\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = np.array(models)\n",
    "\n",
    "new_df['sort_val'] = [np.argwhere(model_order == x.model)[0][0] for x in new_df.iloc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='corr', x='attention_feature', hue='model', data=new_df.sort_values('sort_val'), kind='bar', col='representation', height=6, aspect=1).set_titles('Correlation between attention features and \\nthematic relation signal in \"{col_name}\" representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "results_df.sort_values('corr')[(results_df.experimental_condition=='same_relation_group_rdm_corr') & ((results_df.attention_feature=='modifier_modifier') | (results_df.attention_feature=='head_head') | (results_df.attention_feature=='compound_compound'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = results_df.sort_values('corr', ascending=False)[results_df.experimental_condition=='same_relation_group_rdm_corr_within_compound_sentences']\n",
    "del new_df['experimental_condition']\n",
    "del new_df['self_attention_feature']\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('bert-base-uncased', 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('roberta-base', 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlnet-base-cased', 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlnet-base-cased', 337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlnet-base-cased', 637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('xlnet-base-cased', 37, layers=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('distilroberta-base', 37, layers=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_over_layers('bert-base-japanese', 43, layers=[1, 6, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['attention_target'] = [x.attention_feature.split('_')[0] for x in results_df.iloc]\n",
    "results_df['attention_source'] = [x.attention_feature.split('_')[1] for x in results_df.iloc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.experimental_condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [(x[0], x[1].groupby('attention_target')['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr')].groupby('layer_selection')]\n",
    "# pd.DataFrame([{'layer_selection': x[0], 'mean_compound_corr': x[1]['compound'], 'mean_head_corr': x[1]['head'], 'mean_modifier_corr': x[1]['modifier']} for x in rows])\n",
    "pd.DataFrame([{'layer_selection': x[0], 'mean_head_corr': x[1]['head'], 'mean_modifier_corr': x[1]['modifier']} for x in rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [(x[0], x[1].groupby('attention_source')['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr')].groupby('layer_selection')]\n",
    "pd.DataFrame([{'layer_selection': x[0], 'mean_head_corr': x[1]['head'], 'mean_modifier_corr': x[1]['modifier']} for x in rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr') & (results_df.layer_selection=='all')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.experimental_condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in results_df.experimental_condition.unique():\n",
    "    print(condition)\n",
    "    print('\\t{}'.format([(x[0], x[1]['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition==condition) & (results_df.layer_selection=='all')].groupby('model')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x[0], x[1]['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr') & (results_df.layer_selection=='all')].groupby('model')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x[0], x[1]['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition=='same_relation_group_rdm_corr_within_compound_sentences') & (results_df.layer_selection=='all')].groupby('model')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for condition in [x for x in results_df.experimental_condition.unique() if 'within' in x]:\n",
    "    print(condition)\n",
    "    print('\\t{}'.format([(x[0], x[1]['corr'].mean()) for x in results_df.sort_values('corr', ascending=False)[(results_df.experimental_condition==condition) & (results_df.layer_selection=='all')].groupby('model')]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:black_box_nlp_semantic_similarity] *",
   "language": "python",
   "name": "conda-env-black_box_nlp_semantic_similarity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
